{{Readme generated by Grok X Gemini}}
# AI-Powered Search and Reasoning Assistant

## Overview

This project is an AI-powered assistant designed to process user queries, perform advanced searches, and generate detailed, context-aware responses. It leverages multiple AI models, embeddings for semantic search, and external APIs to provide accurate and comprehensive answers. The system is built with modularity and scalability in mind, utilizing asynchronous programming and advanced natural language processing (NLP) techniques.

The assistant is capable of:
- Determining the user's intent and expertise required to answer a query.
- Generating search queries and fetching relevant data from the web using the Tavily API.
- Encoding documents into embeddings for semantic search with FAISS.
- Providing detailed answers by synthesizing information from retrieved documents.

## Principles

The assistant operates on a multi-step pipeline that integrates various technologies:

1. **Intent and Expertise Detection**: The system uses a language model (Ollama with `deepseek-r1:14b`) to analyze user input and determine the most suitable "identity" and "expertise" for responding.
2. **Search Query Generation**: Based on the user's intent, it generates targeted search queries to retrieve relevant information from the web.
3. **Web Search**: The Tavily API is used to fetch up-to-date information based on the generated queries.
4. **Semantic Search**: Retrieved documents are encoded into embeddings using a pre-trained model (`BAAI/bge-m3`), and a FAISS index is built for efficient similarity search.
5. **Answer Synthesis**: The system combines retrieved information with user intent to generate a final, coherent response using another language model.

This approach ensures that responses are both contextually relevant and informed by real-time data, mimicking human-like reasoning.

## Technical Details

### Dependencies
- **Python Libraries**:
  - `requests`: For making HTTP requests to APIs (Tavily and Ollama).
  - `json` and `re`: For parsing and extracting JSON data from text.
  - `asyncio` and `concurrent.futures`: For asynchronous execution and parallel processing.
  - `transformers` (Hugging Face): For loading and using the `BAAI/bge-m3` model for embeddings.
  - `torch`: For GPU-accelerated tensor computations.
  - `faiss`: For efficient similarity search with vector embeddings.
  - `numpy`: For numerical operations on embeddings.
  - `os` and `dotenv`: For environment variable management.
  - `logging`: For structured logging and debugging.
  - `datetime`: For timestamp generation.

- **External APIs**:
  - **Tavily API**: A search engine API for retrieving web content.
  - **Ollama API**: A local API for running language models like `deepseek-r1:14b` and `google_gemma-3-12b-it-Q6_K_L`.

- **Pre-trained Model**:
  - **BAAI/bge-m3**: A multilingual embedding model from Hugging Face, used for encoding text into dense vectors.

### Key Components

#### 1. Model Loading (`load_or_download_model`)
- **Purpose**: Loads the `BAAI/bge-m3` model either from a local directory or downloads it from Hugging Face if not present.
- **Implementation**: Uses `transformers.AutoTokenizer` and `AutoModel` to handle tokenization and embedding generation. The model is moved to GPU (`cuda`) if available, otherwise runs on CPU.
- **Error Handling**: Logs errors and retries downloading if local loading fails.

#### 2. Intent Detection (`call_ollama` and `extract_json_from_text`)
- **Purpose**: Analyzes user input to determine the assistant's identity and expertise.
- **Implementation**: Queries the Ollama API with a prompt to return a JSON response. The `extract_json_from_text` function handles cases where the response might not be perfectly formatted JSON by using regex to extract valid JSON-like structures.
- **Fallback**: Returns default values (e.g., `{"identity": ["助手"], "expertise": ["通用知识"]}`) if parsing fails.

#### 3. Search Query Generation
- **Purpose**: Creates a list of search queries based on user input and current time.
- **Implementation**: Uses another Ollama call to generate a JSON response with a `"queries"` field. These queries are then passed to the Tavily API.

#### 4. Web Search (`tavily_search` and `async_search`)
- **Purpose**: Fetches relevant web content for each query.
- **Implementation**: Uses `requests.post` to call the Tavily API asynchronously with `asyncio` and `ThreadPoolExecutor`. Results are formatted into readable text blocks.
- **Error Handling**: Returns mock results or error messages if the API key is missing or the request fails.

#### 5. Semantic Search (`encode_text`, `build_faiss_index`, `search_index`)
- **Purpose**: Encodes documents into embeddings and searches for the most relevant ones.
- **Implementation**:
  - `encode_text`: Tokenizes text with `BAAI/bge-m3` and extracts embeddings from the model's last hidden state.
  - `build_faiss_index`: Creates a FAISS HNSW (Hierarchical Navigable Small World) index for fast similarity search. Supports GPU acceleration if available.
  - `search_index`: Normalizes query embeddings and retrieves the top-k most similar documents.

#### 6. Answer Generation
- **Purpose**: Synthesizes a final answer using retrieved documents and user intent.
- **Implementation**: Combines reference materials (answers to exploration queries) with a final prompt sent to the `deepseek-r1:14b` model via Ollama.

### Workflow
1. **User Input**: The script takes a user query via `input()`.
2. **Identity and Expertise**: Determined using Ollama.
3. **Search Queries**: Generated based on intent.
4. **Web Retrieval**: Documents fetched via Tavily API.
5. **Semantic Analysis**: Documents encoded and indexed with FAISS.
6. **Exploration**: Sub-queries answered using retrieved data.
7. **Final Answer**: Synthesized and printed.

### Environment Setup
- **Requirements**: Install dependencies with `pip install -r requirements.txt`.
- **Environment Variables**: Set `TAVILY_API_KEY` in a `.env` file.
- **Ollama Setup**: Run Ollama locally (`http://localhost:11434`) with the required models installed.

### Usage
```bash
python main.py
```
Enter a query when prompted (e.g., "最近的自然灾害有哪些？"), and the script will output a detailed response.

## Limitations
- **Local File I/O**: The script assumes write access to save models, which may not work in all environments (e.g., Pyodide).
- **Network Dependency**: Requires a stable internet connection for Tavily and model downloads.
- **Ollama Dependency**: Assumes a local Ollama instance is running.

## Future Improvements
- Add caching for embeddings and search results.
- Support more embedding models for flexibility.
- Enhance error handling for edge cases.

## Installation

1. **Clone the Repository**:
   ```bash
   git clone https://github.com/czybmx/stupid-search-test.git
   ```

2. **Set Up a Virtual Environment** (optional but recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install Dependencies**:
   ```bash
   pip install requests torch transformers faiss-cpu numpy python-dotenv aiohttp
   ```
   For GPU support, replace `faiss-cpu` with `faiss-gpu` (requires CUDA).

4. **Install Ollama**:
   - Download and install Ollama from [ollama.ai](https://ollama.ai/).
   - Ensure the Ollama server is running locally (`ollama serve`).
   - Pull required models:
     ```bash
     deepseek-r1:14b 
     gemma:12b [But im using google_gemma-3-12b-it-Q6_K_L:latest (from Huggingface GGUF)]
     ```

5. **Set Up Environment Variables**:
   - Create a `.env` file in the project root:
     ```bash
     echo "TAVILY_API_KEY=your-tavily-api-key" > .env
     ```
   - Replace `your-tavily-api-key` with your actual Tavily API key (sign up at [tavily.com](https://tavily.com) to get one).

6. **Download the Embedding Model**:
   - The script automatically downloads the `BAAI/bge-m3` model on first run if not present. Ensure you have an internet connection and sufficient disk space (~2GB).

## Tutorial

### Step 1: Prepare the Environment
After installation, verify that the Ollama server is running:
```bash
ollama serve
```
Check that your `.env` file contains a valid Tavily API key.

### Step 2: Run the Script
Execute the script with:
```bash
python main.py
```
You’ll be prompted to enter a query:
```
User: What are the latest natural disasters in 2025?
```

### Step 3: Understand the Workflow
1. **Input Processing**: The script analyzes your query to determine intent and required expertise.
2. **Search**: It generates search queries and fetches results via Tavily API.
3. **Vector Indexing**: Retrieved documents are encoded and indexed using FAISS.
4. **Answer Generation**: Multiple AI models collaborate to explore the topic and provide a final answer.

### Step 4: View Output
The final answer is printed to the console, prefixed with `===== FINAL ANSWER =====`.

## Usage Methods

### Basic Usage
Run the script and input a question:
```bash
python main.py
User: How does climate change affect agriculture?
```

### Debugging
Enable detailed logging by modifying the script’s logging level:
```python
logging.basicConfig(level=logging.DEBUG)
```
This provides insights into each step (e.g., API calls, model responses).

### Customizing Models
Edit the script to use different Ollama models:
- Change `deepseek-r1:14b` or `google_gemma-3-12b-it-Q6_K_L:latest` in the `call_ollama` function calls.
- Ensure the new models are pulled via `ollama pull <model-name>`.

### Running on GPU
If you have a CUDA-enabled GPU:
1. Install `faiss-gpu` and ensure PyTorch is built with CUDA support.
2. The script auto-detects and uses the GPU if available.

## Troubleshooting
- **Model Download Fails**: Check internet connectivity and disk space in the `models/` directory.
- **Ollama Errors**: Ensure the Ollama server is running (`ollama serve`) and models are pulled.
- **Tavily API Issues**: Verify your API key in `.env` and ensure you have an active subscription.
- **Memory Issues**: Reduce `max_results` in `tavily_search` or use a machine with more RAM.

## Contributing
Feel free to submit pull requests or open issues on GitHub. Contributions to improve performance, add features, or enhance documentation are welcome!

